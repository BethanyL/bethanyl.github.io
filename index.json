[{"authors":["admin"],"categories":null,"content":"Dr. Bethany Lusch is an Assistant Computer Scientist in the data science group at the Argonne Leadership Computing Facility at Argonne National Lab. Her research expertise includes developing methods and tools to integrate AI with science, especially for dynamical systems and PDE-based simulations. Her recent work includes developing machine-learning emulators to replace expensive parts of simulations, such as computational fluid dynamics simulations of engines and climate simulations. She is also working on methods that incorporate domain knowledge in machine learning, representation learning, and using machine learning to analyze supercomputer logs. She holds a PhD and MS in applied mathematics from the University of Washington and a BS in mathematics from the University of Notre Dame.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://bethanyl.github.io/author/bethany-lusch/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bethany-lusch/","section":"authors","summary":"Dr. Bethany Lusch is an Assistant Computer Scientist in the data science group at the Argonne Leadership Computing Facility at Argonne National Lab. Her research expertise includes developing methods and tools to integrate AI with science, especially for dynamical systems and PDE-based simulations.","tags":null,"title":"Bethany Lusch","type":"authors"},{"authors":[],"categories":null,"content":"","date":1591146598,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591146598,"objectID":"5ecd6b06749f0f7202ac3d1680c8fd52","permalink":"https://bethanyl.github.io/talk/mic-2020/","publishdate":"2020-07-02T20:09:58-05:00","relpermalink":"/talk/mic-2020/","section":"talk","summary":"","tags":[],"title":"MIC 2020","type":"talk"},{"authors":[],"categories":null,"content":"","date":1591145272,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591145272,"objectID":"3e14c3bb622360f8800f5ebdecf85a74","permalink":"https://bethanyl.github.io/talk/siam-mathematics-2020/","publishdate":"2020-07-02T19:47:52-05:00","relpermalink":"/talk/siam-mathematics-2020/","section":"talk","summary":"","tags":[],"title":"SIAM Mathematics 2020","type":"talk"},{"authors":["Romit Maulik","Himanshu Sharma","Saumil Patel","Bethany Lusch","Elise Jennings"],"categories":[],"content":"","date":1590117227,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590117227,"objectID":"289f382c88827fa2921cc659ff529b4c","permalink":"https://bethanyl.github.io/publication/accelerating-rans-simulations/","publishdate":"2020-07-02T22:13:47-05:00","relpermalink":"/publication/accelerating-rans-simulations/","section":"publication","summary":"Reynolds-averaged Navier-Stokes (RANS) equations for steady-state assessment of incompressibleturbulent flows remain the workhorse for practical computational fluid dynamics (CFD) applications,and improvements in speed or accuracy have the potential to affect a diverse range of sectors. Weintroduce a machine learning framework for the acceleration of RANS to predict steady-state turbulenteddy viscosities, given the initial conditions. This surrogate model for the turbulent eddy viscosityis assessed for parametric interpolation, while numerically solving for the pressure and velocityequations to steady state, thus representing a framework that is hybridized with machine learning. Weachieve accurate steady-state results with a significant reduction in solution time when compared tothose obtained by the Spalart-Allmaras one-equation model. Most notably the proposed methodologyallows for considerably larger relaxation factors for the steady-state velocity and pressure solvers. Ourassessments are made for a backward-facing step with considerable mesh anisotropy and separationto represent a practical CFD application. For test experiments with varying inlet velocity conditions,we see time-to-solution reductions around a factor of 5. Similar results are obtained for a surrogatemodeling strategy that generalizes across varying step heights. The proposed framework representsan excellent opportunity for the rapid exploration of large parameter spaces that prove prohibitivewhen utilizing turbulence closure models with multiple coupled partial differential equations.","tags":["fluid dynamics","computational physics"],"title":"Accelerating RANS Simulations Using A Data-Driven Framework for Eddy-Viscosity Emulation","type":"publication"},{"authors":["Romit Maulik","Romain Egele","Bethany Lusch","Prasanna Balaprakash"],"categories":[],"content":"","date":1587780309,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587780309,"objectID":"49340c44ff8b7766b35946a30230742b","permalink":"https://bethanyl.github.io/publication/recurrent-neural-network/","publishdate":"2020-07-02T21:05:09-05:00","relpermalink":"/publication/recurrent-neural-network/","section":"publication","summary":"Developing surrogate geophysical models from data is a key research topic in atmospheric andoceanic modeling because of the large computational costs associated with numerical simulationmethods. Researchers have started applying a wide range of machine learning models, in particularneural networks, to geophysical data for forecasting without these constraints. However, constructingneural networks for forecasting such data is nontrivial and often requires trial and error. To that end,we focus on developing proper-orthogonal-decomposition-based long short-term memory networks(POD-LSTMs). We develop a scalable neural architecture search for generating stacked LSTMs toforecast temperature in the NOAA Optimum Interpolation Sea-Surface Temperature data set. Ourapproach identifies POD-LSTMs that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategieson up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.","tags":["LSTMs","AutoML","Emulators","Geophysics"],"title":"Recurrent Neural Network Architecture Search for Geophysical Emulation","type":"publication"},{"authors":[],"categories":null,"content":"","date":1583107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583107200,"objectID":"10703c0422bd9f859926c50f99068f86","permalink":"https://bethanyl.github.io/talk/argonne-ai-hpc-seminar/","publishdate":"2020-07-02T20:10:33-05:00","relpermalink":"/talk/argonne-ai-hpc-seminar/","section":"talk","summary":"","tags":[],"title":"Argonne AI HPC Seminar","type":"talk"},{"authors":["Romit Maulik","Bethany Lusch","Prasanna Balaprakash"],"categories":[],"content":"","date":1580870057,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580870057,"objectID":"d49f48664f1e36d9a57a7e50762963bf","permalink":"https://bethanyl.github.io/publication/reduced-order-modeling/","publishdate":"2020-07-02T21:34:17-05:00","relpermalink":"/publication/reduced-order-modeling/","section":"publication","summary":"A common strategy for the dimensionality reduction of nonlinear partial dif-ferential equations relies on the use of the proper orthogonal decomposition(POD) to identify a reduced subspace and the Galerkin projection for evolv-ing  dynamics  in  this  reduced  space.   However,  advection-dominated  PDEsare represented poorly by this methodology since the process of truncationdiscards important interactions between higher-order modes during time evo-lution.  In this study, we demonstrate that an encoding using convolutionalautoencoders (CAEs) followed by a reduced-space time evolution by recur-rent neural networks overcomes this limitation effectively.  We demonstratethat a truncated system of only two latent-space dimensions can reproduce asharp advecting shock profile for the viscous Burgers equation with very lowviscosities, and a twelve-dimensional latent space can recreate the evolutionof the inviscid shallow water equations.  Additionally,  the proposed frame-work is extended to a parametric reduced-order model by directly embeddingparametric information into the latent space to detect trends in system evo-lution.  Our results show that these advection-dominated systems are moreamenable  to  low-dimensional  encoding  and  time  evolution  by  a  CAE  andrecurrent neural network combination than the POD Galerkin technique.","tags":["ROMs","Autoencoders","Recurrent neural networks"],"title":"Reduced-order modeling of advection-dominatedsystems with recurrent neural networks andconvolutional autoencoders","type":"publication"},{"authors":[],"categories":null,"content":"","date":1580692268,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580692268,"objectID":"cc3c06b812ff3bf4bbe7f891d77c7bd7","permalink":"https://bethanyl.github.io/talk/siam-parallel-processing/","publishdate":"2020-07-02T20:11:08-05:00","relpermalink":"/talk/siam-parallel-processing/","section":"talk","summary":"","tags":[],"title":"SIAM Parallel Processing","type":"talk"},{"authors":["Romit Maulik","Arvind Mohan","Bethany Lusch","Sandeep Madireddy","Prasanna Balaprakash","Daniel Livescu"],"categories":[],"content":"","date":1580178358,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580178358,"objectID":"2e43e47ffe73ed7cf6df329a994c0a6a","permalink":"https://bethanyl.github.io/publication/time-series-learning/","publishdate":"2020-07-02T21:25:58-05:00","relpermalink":"/publication/time-series-learning/","section":"publication","summary":"We study the performance of long short-term memory networks (LSTMs) and neural ordinarydifferential equations (NODEs) in learning latent-space representations of dynamical equations for anadvection-dominated problem given by the viscous Burgers equation. Our formulation is devised in anonintrusive manner with an equation-free evolution of dynamics in a reduced space with the latterbeing obtained through a proper orthogonal decomposition. In addition, we leverage the sequentialnature of learning for both LSTMs and NODEs to demonstrate their capability for closure in systemsthat are not completely resolved in the reduced space. We assess our hypothesis for two advection-dominated problems given by the viscous Burgers equation. We observe that both LSTMs and NODEsare able to reproduce the effects of the absent scales for our test cases more effectively than doesintrusive dynamics evolution through a Galerkin projection. This result empirically suggests that time-series learning techniques implicitly leverage a memory kernel for coarse-grained system closure asis suggested through the Mori–Zwanzig formalism.","tags":["ROMs","LSTMs","Neural ODEs","Closures"],"title":"Time-series learning of latent-space dynamics for reduced-order model closure","type":"publication"},{"authors":["F.N.U. Shilpika","Bethany Lusch","Murali Emani","Venkatram Vishwanath","Michael E. Papka","Kwan Liu Ma"],"categories":[],"content":"","date":1574477050,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574477050,"objectID":"ca0e050827425bbb26d8a836233f780f","permalink":"https://bethanyl.github.io/publication/mela/","publishdate":"2020-07-02T21:44:10-05:00","relpermalink":"/publication/mela/","section":"publication","summary":"To maintain a robust and reliable supercomputing hardware system there is a critical need to understand various system events, including failures occurring in the system. Toward this goal, we analyze various system logs such as error logs, job logs and environment logs from Argonne Leadership Computing Facility's (ALCF) Theta Cray XC40 supercomputer. This log data incorporates multiple subsystem and component measurements at various fidelity levels and temporal resolutions-a very diverse and massive dataset. To effectively identify various patterns that characterize system behavior and faults over time, we have developed a visual analytics tool, MELA, to better identify patterns and glean insights from these log data.","tags":["Clustering","Error Log Analysis","HPC","Topics Over Time","Visualization"],"title":"MELA: A visual analytics tool for studying multifidelity HPC system logs","type":"publication"},{"authors":["Criag Gin","Bethany Lusch","Steven L. Brunton","J. Nathan Kutz"],"categories":[],"content":"","date":1573527408,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573527408,"objectID":"e96daba41976db1530417d3d73fa206f","permalink":"https://bethanyl.github.io/publication/deep-learning-models/","publishdate":"2020-07-02T21:56:48-05:00","relpermalink":"/publication/deep-learning-models/","section":"publication","summary":"We develop a deep autoencoder architecture that can be used to find a coordinate trans-formation which turns a nonlinear PDE into a linear PDE. Our architecture is motivatedby the linearizing transformations provided by the Cole-Hopf transform for Burgers equa-tion and the inverse scattering transform for completely integrable PDEs. By leveraginga  residual  network  architecture,  a  near-identity  transformation  can  be  exploited  to  en-code intrinsic coordinates in which the dynamics are linear. The resulting dynamics aregiven by a Koopman operator matrixK. The decoder allows us to transform back to theoriginal coordinates as well. Multiple time step prediction can be performed by repeatedmultiplication by the matrixKin the intrinsic coordinates. We demonstrate our methodon  a  number  of  examples,  including  the  heat  equation  and  Burgers  equation,  as  wellas the substantially more challenging Kuramoto-Sivashinsky equation, showing that ourmethod provides a robust architecture for discovering interpretable, linearizing transformsfor nonlinear PDEs","tags":["Koopman theory","deep neural nets","residual networks","linearizing transforms","Cole-Hopf transform"],"title":"Deep Learning Models for Global CoordinateTransformations that Linearize PDEs","type":"publication"},{"authors":["Romit Maulik","Vishwas Rao","Sandeep Madireddy","Bethany Lusch","Prasanna Balaprakash"],"categories":[],"content":"","date":1572665658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572665658,"objectID":"52ca9877865f73d51cc2ac39511cd7c5","permalink":"https://bethanyl.github.io/publication/using-recurrent-neural-networks/","publishdate":"2020-07-02T22:34:18-05:00","relpermalink":"/publication/using-recurrent-neural-networks/","section":"publication","summary":"Rapid simulations of advection-dominated problems are vital for multiple engineering and geophysical applications. In this paper, we present a long short-term memory neural network to approximate the nonlinear component of the reduced-order model (ROM) of an advection-dominated partial differential equation. This is motivated by the fact that the nonlinear term is the most expensive component of a successful ROM. For our approach, we utilize a Galerkin projection to isolate the linear and the transient components of the dynamical system and then use discrete empirical interpolation to generate training data for supervised learning. We note that the numerical time-advancement and linear-term computation of the system ensures a greater preservation of physics than does a process that is fully modeled. Our results show that the proposed framework recovers transient dynamics accurately without nonlinear term computations in full-order space and represents a cost-effective alternative to solely equation-based ROMs.","tags":[],"title":"Using recurrent neural networks for nonlinear component computation in advection-dominated reduced-order models","type":"publication"},{"authors":["Kathleen Champion","Bethany Lusch","J. Nathan Kutz","Steven L. Brunton"],"categories":[],"content":"","date":1571713469,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571713469,"objectID":"8f97145522be1f08b9ae0d4726d5e54b","permalink":"https://bethanyl.github.io/publication/data-driven-discovery/","publishdate":"2020-07-02T22:04:29-05:00","relpermalink":"/publication/data-driven-discovery/","section":"publication","summary":"The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.","tags":["model discovery","dynamical systems","machine learning","deep learning"],"title":"Data-driven discovery of coordinates andgoverning equations","type":"publication"},{"authors":["Bethany Lusch","J. Nathan Kutz","Steven L. Brunton"],"categories":[],"content":"","date":1543031326,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543031326,"objectID":"4cb4a9dc258865fcf9c53589121a0feb","permalink":"https://bethanyl.github.io/publication/deep-learning-for-universal-linear-embeddings/","publishdate":"2020-07-02T22:48:46-05:00","relpermalink":"/publication/deep-learning-for-universal-linear-embeddings/","section":"publication","summary":"Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear has the potential to enable nonlinear prediction, estimation, and control using linear theory. The Koopman operator is a leading data-driven embedding, and its eigenfunctions provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven challenging. This work leverages deep learning to discover representations of Koopman eigenfunctions from data. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold. We identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems with continuous spectra. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding, while connecting our models to decades of asymptotics. Thus, we benefit from the power of deep learning, while retaining the physical interpretability of Koopman embeddings.","tags":[],"title":"Deep learning for universal linear embeddings of nonlinear dynamics","type":"publication"},{"authors":["Bethany Lusch","Jake Weholt","Pedro Maia","J. Nathan Kutz"],"categories":[],"content":"","date":1481687603,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481687603,"objectID":"ad6e6c11c8af7600b8a2c7c3e0a98fda","permalink":"https://bethanyl.github.io/publication/modeling-cognitive-deficits/","publishdate":"2020-07-02T22:53:23-05:00","relpermalink":"/publication/modeling-cognitive-deficits/","section":"publication","summary":"The accurate diagnosis and assessment of neurodegenerative disease and traumatic brain injuries (TBI) remain open challenges. Both cause cognitive and functional deficits due to focal axonal swellings (FAS), but it is difficult to deliver a prognosis due to our limited ability to assess damaged neurons at a cellular level in vivo. We simulate the effects of neurodegenerative disease and TBI using convolutional neural networks (CNNs) as our model of cognition. We utilize biophysically relevant statistical data on FAS to damage the connections in CNNs in a functionally relevant way. We incorporate energy constraints on the brain by pruning the CNNs to be less over-engineered. Qualitatively, we demonstrate that damage leads to human-like mistakes. Our experiments also provide quantitative assessments of how accuracy is affected by various types and levels of damage. The deficit resulting from a fixed amount of damage greatly depends on which connections are randomly injured, providing intuition for why it is difficult to predict impairments. There is a large degree of subjectivity when it comes to interpreting cognitive deficits from complex systems such as the human brain. However, we provide important insight and a quantitative framework for disorders in which FAS are implicated.","tags":["traumatic brain injury","neurodegenerative disease","focal axonal swellings","convolutional neural networks"],"title":"Modeling cognitive deficits following neurodegenerative diseases and traumatic brain injuries with deep convolutional neural networks","type":"publication"},{"authors":["Bethany Lusch","Pedro D. Maia","J. Nathan Kutz"],"categories":[],"content":"","date":1475035484,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475035484,"objectID":"e5dbd643d4bb212854a93e94f970c761","permalink":"https://bethanyl.github.io/publication/inferring-connectivity/","publishdate":"2020-07-02T23:04:44-05:00","relpermalink":"/publication/inferring-connectivity/","section":"publication","summary":"Determining the interactions and causal relationships between nodes in an unknown networked dynamical system from measurement data alone is a challenging, contemporary task across the physical, biological, and engineering sciences. Statistical methods, such as the increasingly popular Granger causality, are being broadly applied for data-driven discovery of connectivity in fields from economics to neuroscience. A common version of the algorithm is called pairwise-conditional Granger causality, which we systematically test on data generated from a nonlinear model with known causal network structure. Specifically, we simulate networked systems of Kuramoto oscillators and use the Multivariate Granger Causality Toolbox to discover the underlying coupling structure of the system. We compare the inferred results to the original connectivity for a wide range of parameters such as initial conditions, connection strengths, community structures, and natural frequencies. Our results show a significant systematic disparity between the original and inferred network, unless the true structure is extremely sparse or dense. Specifically, the inferred networks have significant discrepancies in the number of edges and the eigenvalues of the connectivity matrix, demonstrating that they typically generate dynamics which are inconsistent with the ground truth. We provide a detailed account of the dynamics for the Erdős-Rényi network model due to its importance in random graph theory and network science. We conclude that Granger causal methods for inferring network structure are highly suspect and should always be checked against a ground truth model. The results also advocate the need to perform such comparisons with any network inference method since the inferred connectivity results appear to have very little to do with the ground truth system. ","tags":[],"title":"Inferring Connectivity in Networked Dynamical Systems: Challenges Using Granger Causality","type":"publication"},{"authors":["Bethany Lusch","Eric C. Chi","J. Nathan Kutz"],"categories":[],"content":"","date":1471404030,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471404030,"objectID":"403245d6b82fdae69718f2e64aba8fe2","permalink":"https://bethanyl.github.io/publication/shape-constrained-tensor-decompositions/","publishdate":"2020-07-02T22:20:30-05:00","relpermalink":"/publication/shape-constrained-tensor-decompositions/","section":"publication","summary":"We consider N-way data arrays and low-rank tensor factorizations  where  the  time  mode  is  coded  as  a  sparse  linear combination of temporal elements from an over-complete library. Our  method,  Shape  Constrained  Tensor  Decomposition  (SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which   produces r-rank   approximations   of   data   tensors   via outer  products  of  vectors  in  each  dimension  of  the  data.  By constraining  the  vector  in  the  temporal  dimension  to  known analytic  forms  which  are  selected  from  a  large  set  of  candidate functions, more readily interpretable decompositions are achieved and  analytic  time  dependencies  discovered.  The  SCTD  method circumvents  traditional flattening techniques  where  an N-way array  is  reshaped  into  a  matrix  in  order  to  perform  a  singular value  decomposition.  A  clear  advantage  of  the  SCTD  algorithmis  its  ability  to  extract  transient  and  intermittent  phenomena which is often difficult for SVD-based methods. We motivate the SCTD  method  using  several  intuitively  appealing  results  before applying  it  on  a  number  of  high-dimensional,  real-world  datasets  in  order  to  illustrate  the  efficiency  of  the  algorithm  inextracting  interpretable  spatio-temporal  modes.  With  the  rise of  data-driven  discovery  methods,  the  decomposition  proposed provides  a  viable  technique  for  analyzing  multitudes  of  data  in a  more  comprehensible  fashion.","tags":["machine learning","tensor decomposition","multiway arrays","multilinear algebra","higher-order singular value decomposition (HOSVD)","over-complete libraries","sparse regression"],"title":"Shape Constrained Tensor Decompositions usingSparse Representations in Over-Complete Libraries","type":"publication"},{"authors":["Jennifer Gillenwater","Rishabh Iyer","Bethany Lusch","Rahul Kidambi","Jeff Bilmes"],"categories":[],"content":"","date":1446523896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446523896,"objectID":"6f6b9b191eb3023b270d645904e530b9","permalink":"https://bethanyl.github.io/publication/submodular-hamming-metrics/","publishdate":"2020-07-02T23:11:36-05:00","relpermalink":"/publication/submodular-hamming-metrics/","section":"publication","summary":"We show that there is a largely unexplored class of functions (positive polyma-troids) that can define proper discrete metrics over pairs of binary vectors andthat are fairly tractable to optimize over.  By exploiting submodularity, we areable to give hardness results and approximation algorithms for optimizing oversuch metrics. Additionally, we demonstrate empirically the effectiveness of thesemetrics and associated algorithms on both a metric minimization task (a form ofclustering) and also a metric maximization task (generating diversek-best lists).","tags":[],"title":"Submodular Hamming Metrics","type":"publication"}]